**************************************************
List of ops (nodes) in the graph
	Op name: conv0/Convolution
	Op name: conv0/TypeCastOp
	Op name: conv0/MaxPooling
	Op name: conv1/Convolution
	Op name: conv1/TypeCastOp
	Op name: conv1/MaxPooling
	Op name: fc0/Convolution
	Op name: fc0/TypeCastOp
	Op name: fc1/Convolution
	Op name: fc1/TypeCastOp
**************************************************
**************************************************
List of tensors (edges) in the graph
	inputs/data[1,8,8,3] (FXP16 (8,8))
	conv0/weights[1,3,3,3] (FXP16 (4,12))
	conv0/biases[1] (FXP32 (12,20))
	conv0/Convolution[1,8,8,1] (FXP64 (44,20))
	conv0/TypeCastOp[1,8,8,1] (FXP16 (8,8))
	conv0/MaxPooling[1,4,4,1] (FXP16 (8,8))
	conv1/weights1[1,3,3,1] (FXP16 (4,12))
	conv1/biases1[1] (FXP32 (12,20))
	conv1/Convolution[1,4,4,1] (FXP64 (44,20))
	conv1/TypeCastOp[1,4,4,1] (FXP16 (8,8))
	conv1/MaxPooling[1,2,2,1] (FXP16 (8,8))
	fc0/fcin[1,1,1,16] (FXP16 (8,8))
	fc0/weights1[6,1,1,16] (FXP16 (4,12))
	fc0/biases1[6] (FXP32 (12,20))
	fc0/Convolution[1,1,1,6] (FXP64 (44,20))
	fc0/TypeCastOp[1,1,1,6] (FXP16 (8,8))
	fc1/fcin[1,1,1,8] (FXP16 (8,8))
	fc1/weights1[2,1,1,8] (FXP16 (4,12))
	fc1/biases1[2] (FXP32 (12,20))
	fc1/Convolution[1,1,1,2] (FXP64 (44,20))
	fc1/TypeCastOp[1,1,1,2] (FXP16 (8,8))
**************************************************
Accelerator object
	Precision: 16
	Systolic array size: 4 -rows x 4 -columns
	IBUF size:    8,192.0 Bytes
	WBUF size:    8,192.0 Bytes
	OBUF size:   32,768.0 Bytes
	BBUF size:   16,384.0 Bytes
Double buffering enabled. Sizes of SRAM are halved
MS :GraphCOmpile Done!!
INFO:Graph Compiler:MS : In Layer :conv0/Convolution
INFO:Graph Compiler:MS : Addr -- Data : 0x0 	 Weights : 0x1000
INFO:Graph Compiler:MS : Addr -- Bias : 0x2000 	 Outputs : 0x3000
MS: b: 1, ic: 1, oc: 1, oh: 8, ow: 8, kh: 3,kw:3
operation is <dnnweaver2.tensorOps.cnn.TypeCastOp object at 0x7f26fc447c88>
operation is <dnnweaver2.tensorOps.cnn.MaxPooling object at 0x7f26fc447d30>
MS. In pu_compile. Alloc size: conv0/MaxPooling[1,4,4,1] (FXP16 (8,8))
MS. Before t_out_addr: 0x6000, len:4,pad_offset:28.0
MS. FInal t_out_addr: 0x6038, pad_offset:56
OBUF.pop                  ->   R0
R0         >>  #(12)      ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         -> ST-DDR.push
INFO:Graph Compiler:MS : In Layer :conv1/Convolution
INFO:Graph Compiler:MS : Addr -- Data : 0x6000 	 Weights : 0x7000
INFO:Graph Compiler:MS : Addr -- Bias : 0x8000 	 Outputs : 0x9000
MS: b: 1, ic: 1, oc: 1, oh: 4, ow: 4, kh: 3,kw:3
operation is <dnnweaver2.tensorOps.cnn.TypeCastOp object at 0x7f26fc456128>
operation is <dnnweaver2.tensorOps.cnn.MaxPooling object at 0x7f26fc4561d0>
MS. In pu_compile. Alloc size: conv1/MaxPooling[1,2,2,1] (FXP16 (8,8))
MS. Before t_out_addr: 0xb000, len:4,pad_offset:0.0
MS. FInal t_out_addr: 0xb000, pad_offset:0
OBUF.pop                  ->   R0
R0         >>  #(12)      ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         -> ST-DDR.push
INFO:Graph Compiler:MS : In Layer :fc0/Convolution
INFO:Graph Compiler:MS : Addr -- Data : 0xb000 	 Weights : 0xc000
INFO:Graph Compiler:MS : Addr -- Bias : 0xd000 	 Outputs : 0xe000
MS: b: 1, ic: 4, oc: 2, oh: 1, ow: 1, kh: 1,kw:1
operation is <dnnweaver2.tensorOps.cnn.TypeCastOp object at 0x7f26fc456630>
MS. In pu_compile. Alloc size: fc0/TypeCastOp[1,1,1,6] (FXP16 (8,8))
MS. Before t_out_addr: 0xf000, len:4,pad_offset:0.0
MS. FInal t_out_addr: 0xf000, pad_offset:0
OBUF.pop                  ->   R0
R0         >>  #(12)      ->   R0
R0                        -> ST-DDR.push
INFO:Graph Compiler:MS : In Layer :fc1/Convolution
INFO:Graph Compiler:MS : Addr -- Data : 0xf000 	 Weights : 0x10000
INFO:Graph Compiler:MS : Addr -- Bias : 0x11000 	 Outputs : 0x12000
MS: b: 1, ic: 2, oc: 1, oh: 1, ow: 1, kh: 1,kw:1
operation is <dnnweaver2.tensorOps.cnn.TypeCastOp object at 0x7f26fc456a90>
MS. In pu_compile. Alloc size: fc1/TypeCastOp[1,1,1,2] (FXP16 (8,8))
MS. Before t_out_addr: 0x13000, len:4,pad_offset:0.0
MS. FInal t_out_addr: 0x13000, pad_offset:0
OBUF.pop                  ->   R0
R0         >>  #(12)      ->   R0
R0                        -> ST-DDR.push
Number of instructions: 346
[   75497472 -1879048192 -1845489664 -1828708352 -1862258688 -1876951040
 -1843396608 -1826619392 -1860173824  1913651200  1611661312  1628438528
  1630535680  1645215744  1661992960   270598145  1881210889  1610678280
  1881210889  1610678352   291635201   560070657  1883373575  1627521056
  1629618208  1883373575  1627521280  1629618432   304283649  1885536258
  1644363808  1885536258  1644363872   323223553  1887698944  1661206528
  1879048194  1614807041  1631584256  1633681408  1648361473  1665138688
  1879048194  1614807050  1631584256  1633681408  1648361475  1665138688
  1879048199  1614807041  1631584257  1633681409  1648361472  1665138688
  1879048199  1614807050  1631584264  1633681416  1648361472  1665138688
 -1610612698 -1879048192 -1862246400 -1860173824  1879048193  1610612737
  1879048193  1610612744  1879048195  1610612738  1879048195  1610612752
  1879048192  1610612737  1879048192  1610612800  1889861632  1704984576
  1723858944  1742733312  1881210883  1629487105  1881210883  1629487110
  1881210880  1629487105  1881210880  1629487140 -1342177152  -822080512
 -1342177151  -822080495 -1258291184 -1342177151  -822080495 -1258291184
 -1342177151  -822080495 -1258291176 -2147483633 -2147483648    75497472
 -1879023616 -1845465088 -1828683776 -1862234112 -1876951040 -1843396608
 -1826619392 -1860173824  1913651200  1611661312  1628438528  1630535680
  1645215744  1661992960   270598145  1881210885  1610678280  1881210885
  1610678320   291635201   560070657  1883373571  1627521056  1629618208
  1883373571  1627521152  1629618304   304283649  1885536258  1644363808
  1885536258  1644363872   323223553  1887698944  1661206528  1879048194
  1614807041  1631584256  1633681408  1648361473  1665138688  1879048194
  1614807046  1631584256  1633681408  1648361475  1665138688  1879048195
  1614807041  1631584257  1633681409  1648361472  1665138688  1879048195
  1614807046  1631584260  1633681412  1648361472  1665138688 -1610612698
 -1879048192 -1862225920 -1860173824  1879048193  1610612737  1879048193
  1610612740  1879048193  1610612738  1879048193  1610612744  1879048192
  1610612737  1879048192  1610612752  1889861632  1704984576  1723858944
  1742733312  1881210881  1629487105  1881210881  1629487106  1881210880
  1629487105  1881210880  1629487108 -1342177152  -822080512 -1342177151
  -822080495 -1258291184 -1342177151  -822080495 -1258291184 -1342177151
  -822080495 -1258291176 -2147483645 -2147483648    75497472 -1879003136
 -1845444608 -1828663296 -1862213632 -1876951040 -1843396608 -1826619392
 -1860173824  1913651200  1611661312  1628438528  1630535680  1645215744
  1661992960   270598145  1881210883  1610678280   291635201   560070657
  1883373569  1627521056  1629618208   304283649  1885536259  1644363808
  1885536257  1644363904   323223553  1887698945  1661206544  1879048195
  1614807041  1631584256  1633681408  1648361473  1665138688  1879048193
  1614807040  1631584257  1633681409  1648361476  1665138689 -1610612706
 -1879048192 -1862209536 -1860173824  1879048192  1610612738  1879048192
  1610612738  1879048192  1610612738  1879048192  1610612738  1879048193
  1610612737  1879048192  1610612738  1889861632  1704984576  1723858944
  1742733312  1881210880  1629487106  1881210880  1629487106  1881210881
  1629487105  1881210880  1629487106 -1342177152  -822080512 -1342177272
 -2147483647 -2147483648    75497472 -1878986752 -1845428224 -1828646912
 -1862197248 -1876951040 -1843396608 -1826619392 -1860173824  1913651200
  1611661312  1628438528  1630535680  1645215744  1661992960   270598145
  1881210881  1610678280   291635201   560070657  1883373568  1627521024
  1629618176   304283649  1885536257  1644363808   323223553  1887698944
  1661206528  1879048193  1614807041  1631584256  1633681408  1648361473
  1665138688 -1610612706 -1879048192 -1862193152 -1860173824  1879048192
  1610612737  1879048192  1610612737  1879048192  1610612737  1879048192
  1610612737  1879048192  1610612737  1879048192  1610612737  1889861632
  1704984576  1723858944  1742733312  1881210880  1629487105  1881210880
  1629487105  1881210880  1629487105  1881210880  1629487105 -1342177152
  -822080512 -1342177272 -2147483648 -2147483647]
