Folders
- rtl : rtl files for DNNW2
- tb  : testbench setup
- data: Image and weight files for targetted Model

Modifications:

 To Bypass PCI interface, following files have been created/modified
- rtl/mxv/controller_noPCI.v  : Copy of controller_noPCI.v with changes made to bypass PCI i/F
- rtl/mxv/instruction_memory_noPCI.v : Copy on instruction_memory.v with changed made to preload "instruction.bin" into instr mem
- rtl/rtl/dnnweaver2_controller.v : `define NO_PCI added to ensure that controller_noPCI.v & instruction_memory_noPCI.v modules are used

Test Environment Setup:

The DNNW2 RTL code has 5 AXI interfaces accessing DDR Memory.
In hardware implementation, the block design ensures that the 5 AXI channels connect to DDR through AXI Interconnect

Testbench Workaround 
- 5 instances of "tb/axi_master_tb_driver.v" is made in testbench, one for each AXI interface of DNNW2_controller
- All 5 instances have their own instance of BRAM (mimicing DDR Memory)
- Based on compiler run.log, the base addresses for different layers are updated.
- @t0, each BRAM instance is initialized with same data(img& weight)
- Since in a Model with multiple layers, the output of one layer acts as input to next layer,for testing purpose, a signal called "sel" is declared in tb_dnnw2_ctrl.vi & set to "0".
- The output of current layer drives the AXI write channel for both pubuf_axim_driver & ibuf_axim_driver. This ensures that the data of previous layer is available to be read from IBUF for next layer.

Simulation Flow:
1. The instructions are generated by Compiler layer-by-layer and written into "instruction.bin".
   (which is used to initilize intruction mem)
2.  The testbench drives signal "start" indicating that DNNW2Ctrl can start operation 
    Note : In hardware implementation this is configured by PCI AXI4Lite Slave. 
           "start" should be generated once img & weights loaded to DDR
3.  The Decoder module gets enabled by "start" signal. It starts reading & decoding instructions preloaded in instruction memory.
4.  Based on instruction appropriate registers (IBUF,WBUF,BBUF,OBUF base addresses, strides, LoopIteration Values etc) are configured.
5.  Once the Systolic Array related instructions are complete,PU related instruction are present in instruction.bin.
6.  The instruction decoder generates "pu_start" signal & asserts "cfg_pu_inst_v" signal for "pu_num_instructions" times
    (Extracted from PU start instruction), PU related signals are  written into instruction memory inside PUGen module.
7.  Once all instructions for a layer have been decoded (once BlockEnd instruction received),
    the decoder drives the "base_loop_ctrl_start" signal of "controller_noPCI.v" block. 
    This indicates that the configuration for the "Layer" is complete
8.  The next step is to fetch the image & weight data from DDR to IBUF(through cl_ddr0 ReadChnl) and 
    DDR to WBUF (through cl_ddr1 ReadChnl).
9.  Once the data is loaded into On-Chip memory "compute_req" signal is generated (dnnweaver2_controller.v") 
    is enabled to perform Conv computation in SystolicArray.  
10. Once Conv computation done and written into OBUF, "compute_req" is deasserted, "compute_done" asserted( dnnweaver2_controller.v)
11. "pu_compute_start" signal is then enabled to execute MaxPool,RelU etc.
    "pu_compute_done" is generated once the Final Layer Output is written into DDR through(cl_ddr4* write channel)
12. If there are multiple Tiles in Layer, data for next tile is fetched from DDR while current tile is being processed.
13. Steps 9-12 are repeated for each tile in Layer. Once all tiles are processed, block_done( controller_noPCI.v) is generated.
14. Above steps are repeated for all Layers. Once last layer is processed, "done" signal is generated by Accelerator

