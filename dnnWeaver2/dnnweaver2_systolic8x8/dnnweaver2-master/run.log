**************************************************
List of ops (nodes) in the graph
	Op name: conv0/Convolution
	Op name: conv0/TypeCastOp
	Op name: conv0/MaxPooling
	Op name: conv1/Convolution
	Op name: conv1/TypeCastOp
	Op name: conv1/MaxPooling
	Op name: fc0/Convolution
	Op name: fc0/TypeCastOp
	Op name: fc1/Convolution
	Op name: fc1/TypeCastOp
**************************************************
**************************************************
List of tensors (edges) in the graph
	inputs/data[1,16,16,3] (FXP16 (8,8))
	conv0/weights[1,5,5,3] (FXP16 (4,12))
	conv0/biases[1] (FXP32 (12,20))
	conv0/Convolution[1,16,16,1] (FXP64 (44,20))
	conv0/TypeCastOp[1,16,16,1] (FXP16 (8,8))
	conv0/MaxPooling[1,8,8,1] (FXP16 (8,8))
	conv1/weights1[1,5,5,1] (FXP16 (4,12))
	conv1/biases1[1] (FXP32 (12,20))
	conv1/Convolution[1,8,8,1] (FXP64 (44,20))
	conv1/TypeCastOp[1,8,8,1] (FXP16 (8,8))
	conv1/MaxPooling[1,4,4,1] (FXP16 (8,8))
	fc0/fcin[1,1,1,128] (FXP16 (8,8))
	fc0/weights1[10,1,1,128] (FXP16 (4,12))
	fc0/biases1[10] (FXP32 (12,20))
	fc0/Convolution[1,1,1,10] (FXP64 (44,20))
	fc0/TypeCastOp[1,1,1,10] (FXP16 (8,8))
	fc1/fcin[1,1,1,16] (FXP16 (8,8))
	fc1/weights1[4,1,1,16] (FXP16 (4,12))
	fc1/biases1[4] (FXP32 (12,20))
	fc1/Convolution[1,1,1,4] (FXP64 (44,20))
	fc1/TypeCastOp[1,1,1,4] (FXP16 (8,8))
**************************************************
Accelerator object
	Precision: 16
	Systolic array size: 8 -rows x 8 -columns
	IBUF size:   16,384.0 Bytes
	WBUF size:   32,768.0 Bytes
	OBUF size:   65,536.0 Bytes
	BBUF size:   32,768.0 Bytes
Double buffering enabled. Sizes of SRAM are halved
MS :GraphCOmpile Done!!
INFO:Graph Compiler:MS : In Layer :conv0/Convolution
INFO:Graph Compiler:MS : Addr -- Data : 0x0 	 Weights : 0x7000
INFO:Graph Compiler:MS : Addr -- Bias : 0xb000 	 Outputs : 0xc000
MS: b: 1, ic: 1, oc: 1, oh: 16, ow: 16, kh: 5,kw:5
operation is <dnnweaver2.tensorOps.cnn.TypeCastOp object at 0x7fa5befc1630>
operation is <dnnweaver2.tensorOps.cnn.MaxPooling object at 0x7fa5befc15f8>
MS. In pu_compile. Alloc size: conv0/MaxPooling[1,8,8,1] (FXP16 (8,8))
MS. Before t_out_addr: 0x1d000, len:4,pad_offset:208.0
MS. FInal t_out_addr: 0x1d1a0, pad_offset:416
OBUF.pop                  ->   R0
R0         >>  #(12)      ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         -> ST-DDR.push
INFO:Graph Compiler:MS : In Layer :conv1/Convolution
INFO:Graph Compiler:MS : Addr -- Data : 0x1d000 	 Weights : 0x20000
INFO:Graph Compiler:MS : Addr -- Bias : 0x24000 	 Outputs : 0x25000
MS: b: 1, ic: 1, oc: 1, oh: 8, ow: 8, kh: 5,kw:5
operation is <dnnweaver2.tensorOps.cnn.TypeCastOp object at 0x7fa5befc1a58>
operation is <dnnweaver2.tensorOps.cnn.MaxPooling object at 0x7fa5befc1b00>
MS. In pu_compile. Alloc size: conv1/MaxPooling[1,4,4,1] (FXP16 (8,8))
MS. Before t_out_addr: 0x2a000, len:4,pad_offset:0.0
MS. FInal t_out_addr: 0x2a000, pad_offset:0
OBUF.pop                  ->   R0
R0         >>  #(12)      ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         ->   R0
OBUF.pop                  ->   R1
R1         >>  #(12)      ->   R1
R1         max R0         -> ST-DDR.push
INFO:Graph Compiler:MS : In Layer :fc0/Convolution
INFO:Graph Compiler:MS : Addr -- Data : 0x2a000 	 Weights : 0x2b000
INFO:Graph Compiler:MS : Addr -- Bias : 0x2f000 	 Outputs : 0x30000
MS: b: 1, ic: 16, oc: 1, oh: 1, ow: 1, kh: 1,kw:1
MS: Store Instrn. Stride:0.0
MS: Store Instrn. Stride:64.0
MS: Store Instrn. Stride:2048.0
MS: Store Instrn. Stride:32.0
operation is <dnnweaver2.tensorOps.cnn.TypeCastOp object at 0x7fa5befc1f98>
MS. In pu_compile. Alloc size: fc0/TypeCastOp[1,1,1,10] (FXP16 (8,8))
MS. Before t_out_addr: 0x31000, len:4,pad_offset:0.0
MS. FInal t_out_addr: 0x31000, pad_offset:0
OBUF.pop                  ->   R0
R0         >>  #(12)      ->   R0
R0                        -> ST-DDR.push
INFO:Graph Compiler:MS : In Layer :fc1/Convolution
INFO:Graph Compiler:MS : Addr -- Data : 0x31000 	 Weights : 0x32000
INFO:Graph Compiler:MS : Addr -- Bias : 0x33000 	 Outputs : 0x34000
MS: b: 1, ic: 2, oc: 1, oh: 1, ow: 1, kh: 1,kw:1
operation is <dnnweaver2.tensorOps.cnn.TypeCastOp object at 0x7fa5befcc400>
MS. In pu_compile. Alloc size: fc1/TypeCastOp[1,1,1,4] (FXP16 (8,8))
MS. Before t_out_addr: 0x35000, len:4,pad_offset:0.0
MS. FInal t_out_addr: 0x35000, pad_offset:0
OBUF.pop                  ->   R0
R0         >>  #(12)      ->   R0
R0                        -> ST-DDR.push
Number of instructions: 338
[   75497472 -1879048192 -1845465088 -1828671488 -1862221824 -1876951040
 -1843396608 -1826619392 -1860173824  1913651200  1611661312  1628438528
  1630535680  1645215744  1661992960   270598145  1881210899  1610678288
  1881210899  1610678592   291635201   560070657  1883373583  1627521088
  1629618240  1883373583  1627522048  1629619200   304283649  1885536260
  1644363904  1885536260  1644364416   323223553  1887698944  1661206528
  1879048196  1614807041  1631584256  1633681408  1648361473  1665138688
  1879048196  1614807060  1631584256  1633681408  1648361477  1665138688
  1879048207  1614807041  1631584257  1633681409  1648361472  1665138688
  1879048207  1614807060  1631584272  1633681424  1648361472  1665138688
 -1610612698 -1879048192 -1862152192 -1860173824  1879048193  1610612737
  1879048193  1610612752  1879048199  1610612738  1879048199  1610612768
  1879048192  1610612737  1879048192  1610612992  1889861632  1704984576
  1723858944  1742733312  1881210887  1629487105  1881210887  1629487116
  1881210880  1629487105  1881210880  1629487248 -1342177152  -822080512
 -1342177151  -822080495 -1258291184 -1342177151  -822080495 -1258291184
 -1342177151  -822080495 -1258291176 -2147483585 -2147483648    75497472
 -1878929408 -1845362688 -1828569088 -1862119424 -1876951040 -1843396608
 -1826619392 -1860173824  1913651200  1611661312  1628438528  1630535680
  1645215744  1661992960   270598145  1881210891  1610678288  1881210891
  1610678464   291635201   560070657  1883373575  1627521088  1629618240
  1883373575  1627521536  1629618688   304283649  1885536260  1644363904
  1885536260  1644364416   323223553  1887698944  1661206528  1879048196
  1614807041  1631584256  1633681408  1648361473  1665138688  1879048196
  1614807052  1631584256  1633681408  1648361477  1665138688  1879048199
  1614807041  1631584257  1633681409  1648361472  1665138688  1879048199
  1614807052  1631584264  1633681416  1648361472  1665138688 -1610612698
 -1879048192 -1862098944 -1860173824  1879048193  1610612737  1879048193
  1610612744  1879048195  1610612738  1879048195  1610612752  1879048192
  1610612737  1879048192  1610612800  1889861632  1704984576  1723858944
  1742733312  1881210883  1629487105  1881210883  1629487108  1881210880
  1629487105  1881210880  1629487120 -1342177152  -822080512 -1342177151
  -822080495 -1258291184 -1342177151  -822080495 -1258291184 -1342177151
  -822080495 -1258291176 -2147483633 -2147483648    75497472 -1878876160
 -1845317632 -1828524032 -1862074368 -1876951040 -1843396608 -1826619392
 -1860173824  1913651201  1611661312  1628438592  1630535744  1645217792
  1661992992   270598145  1881210895  1610678288   291635201   560070657
  1883373568  1627521024  1629618176   304283649  1885536271  1644363904
   323223553  1887698944  1661206528  1879048207  1614807041  1631584256
  1633681408  1648361473  1665138688 -1610612706 -1879048192 -1862070272
 -1860173824  1879048192  1610612737  1879048192  1610612737  1879048192
  1610612737  1879048192  1610612737  1879048192  1610612737  1879048192
  1610612737  1889861633  1704984592  1723858944  1742733312  1881210880
  1629487106  1881210880  1629487106  1881210880  1629487105  1881210880
  1629487106 -1342177152  -822080512 -1342177272 -2147483648 -2147483648
    75497472 -1878847488 -1845288960 -1828507648 -1862057984 -1876951040
 -1843396608 -1826619392 -1860173824  1913651200  1611661312  1628438528
  1630535680  1645215744  1661992960   270598145  1881210881  1610678288
   291635201   560070657  1883373568  1627521024  1629618176   304283649
  1885536257  1644363904   323223553  1887698944  1661206528  1879048193
  1614807041  1631584256  1633681408  1648361473  1665138688 -1610612706
 -1879048192 -1862053888 -1860173824  1879048192  1610612737  1879048192
  1610612737  1879048192  1610612737  1879048192  1610612737  1879048192
  1610612737  1879048192  1610612737  1889861632  1704984576  1723858944
  1742733312  1881210880  1629487105  1881210880  1629487105  1881210880
  1629487105  1881210880  1629487105 -1342177152  -822080512 -1342177272
 -2147483648 -2147483647]
